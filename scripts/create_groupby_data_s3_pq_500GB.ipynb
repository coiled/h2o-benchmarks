{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99e6d4-dca3-481e-8e25-856281071792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, wait\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867d45a-c1d0-4000-8310-43ff6e9bcc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45848eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr as zr\n",
    "#from dask.sizeof import sizeof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c8f2e",
   "metadata": {},
   "source": [
    "## Save sample_id3 as array in s3\n",
    "\n",
    "Turns out that for the case of `N=1e10`, `sample_id3 = [f\"id{str(x).zfill(10)}\" for x in range(1, int(N / K) + 1)]` is a list of ~7GB. Producing this list in the workers is not only slow but also puts a lot of preassure on the memory. \n",
    "\n",
    "Doing a quick test, converting this list to a numpy array result in a 4.5GB array and\n",
    "then smapling from that array is faster than sampling from the list. \n",
    "\n",
    "Since this array does not change accross partitons, we wrote the array with zarr in S3 and then we read it in the workers. \n",
    "\n",
    "### Array creation \n",
    "\n",
    "This code was run to create the array, the following is for documentation purposes.\n",
    "Needs zarr installed.\n",
    "```python\n",
    "client = Client()\n",
    "\n",
    "N = int(1e10)\n",
    "K = int(1e2)\n",
    "sample_id3_list = [f\"id{str(x).zfill(10)}\" for x in range(1, int(N / K) + 1)]\n",
    "sample_id3_np = np.array(sample_id3_list)\n",
    "sample_id3_da =  da.from_array(sample_id3_np)\n",
    "\n",
    "dir_h2o = \"s3://coiled-datasets/h2o-benchmark/sample_id3_arr\"\n",
    "da.to_zarr(sample_id3_da, dir_h2o)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c2cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dask.delayed\n",
    "# def create_sample_id3():\n",
    "#     x = zr.open(\"s3://coiled-datasets/h2o-benchmark/sample_id3_arr\")\n",
    "#     return x[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d51a9-5f7e-467b-8061-a1266edfc2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@dask.delayed\n",
    "def create_single_df(N, K, nfiles, dir, i):\n",
    "    \"\"\"\n",
    "    Creates a single pandas dataframe that contains nrows=N/nfiles\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    N: int,\n",
    "     Total number of rows\n",
    "    K: int,\n",
    "     Number of groups\n",
    "    nfiles: int,\n",
    "     Number of output files\n",
    "    dir: str,\n",
    "     Output directory\n",
    "    i: int,\n",
    "     Integer to assign to the multiple files e.g. range(nfiles)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    nrows = int(N / nfiles)\n",
    "    print(nrows)\n",
    "\n",
    "    sample_id12 = [f\"id{str(x).zfill(3)}\" for x in range(1, K + 1)]\n",
    "    \n",
    "    #sample_id3 = da.from_zarr(\"s3://coiled-datasets/h2o-benchmark/sample_id3_arr\").compute()\n",
    "    \n",
    "    sample_id3 = zr.open(\"s3://coiled-datasets/h2o-benchmark/sample_id3_arr\")[:]\n",
    "    \n",
    "    id1 = np.random.choice(sample_id12, size=nrows, replace=True)\n",
    "    id2 = np.random.choice(sample_id12, size=nrows, replace=True)\n",
    "    id3 = np.random.choice(sample_id3, size=nrows, replace=True)\n",
    "    id4 = np.random.choice(K, size=nrows, replace=True)\n",
    "    id5 = np.random.choice(K, size=nrows, replace=True)\n",
    "    id6 = np.random.choice(int(N / K), size=nrows, replace=True)\n",
    "    v1 = np.around(np.random.choice(5, size=nrows, replace=True), decimals=6)\n",
    "    v2 = np.around(np.random.choice(15, size=nrows, replace=True), decimals=6)\n",
    "    v3 = np.around(np.random.uniform(0, 100, size=nrows), decimals=6)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        dict(\n",
    "            zip(\n",
    "                [f\"id{x}\" for x in range(1, 7)] + [\"v1\", \"v2\", \"v3\"],\n",
    "                [id1, id2, id3, id4, id5, id6, v1, v2, v3],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = df.astype({\n",
    "            \"id1\": \"string[pyarrow]\", #this was category before\n",
    "            \"id2\": \"string[pyarrow]\", #this was category before\n",
    "            \"id3\": \"string[pyarrow]\", #this was category before\n",
    "            \"id4\": \"Int32\",\n",
    "            \"id5\": \"Int32\",\n",
    "            \"id6\": \"Int32\",\n",
    "            \"v1\": \"Int32\",\n",
    "            \"v2\": \"Int32\",\n",
    "            \"v3\": \"float64\",\n",
    "        })\n",
    "\n",
    "    N_pretty = ''.join(f\"{N:.0E}\".split(\"+\"))\n",
    "    K_pretty = ''.join(f\"{K:.0E}\".split(\"+0\"))\n",
    "\n",
    "    df.to_parquet(\n",
    "        f\"{dir}/groupby-N_{N_pretty}_K_{K_pretty}_file_{i}.parquet\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d09b4",
   "metadata": {},
   "source": [
    "### Cluster details\n",
    "\n",
    "We use 1 thread and m6i.xlarge (16GiB per worker) because we need enough memory to read the sample_id3 array, it's a 4.8GB array. We can get away with just 8GiB, but there is a bit of spilling. \n",
    "\n",
    "With 100 workers, we can write the whole dataset in ~5.5 min.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a6172f-deb5-4079-9169-5a7cf8310786",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(n_workers=100,\n",
    "                         name=\"create_groupby_500\",\n",
    "                         scheduler_vm_types=\"m6i.xlarge\",\n",
    "                         worker_vm_types = \"m6i.xlarge\", \n",
    "                         worker_options={\"nthreads\": 1}, #16GB per worker\n",
    "                         package_sync=True,\n",
    "                         scheduler_options={\"idle_timeout\": \"2 hour\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ffdca2-ddec-44f4-a1ab-f3d25c7d0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a2d7f-acea-40b5-b161-274a997e0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1e10\n",
    "K = 1e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed5ee0-0bbe-4599-9a98-e6a4103e21ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_dir = \"s3://coiled-datasets/h2o-benchmark/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882532e0-ccea-4284-8776-c7ed60ca603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(N)\n",
    "K = int(K)\n",
    "nfiles = 5000 #this creates pq files that in memory take 109MB\n",
    "dir = s3_dir + \"pyarrow_strings/N_1e10_K_1e2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121c980-1326-44b0-9244-ff5ecc7b92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "futures = client.map(\n",
    "            lambda i: create_single_df(N, K, nfiles, dir, i), range(nfiles), \n",
    "            pure=False\n",
    "        )\n",
    "wait(futures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
