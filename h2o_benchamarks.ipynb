{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f479607",
   "metadata": {},
   "source": [
    "## Grouby queries from h2o-benchmarks (parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b24ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import coiled\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, performance_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this cluster might be small for the 50GB \n",
    "cluster = coiled.Cluster(\n",
    "    name=\"h2o-benchmarks\",\n",
    "    n_workers=10,\n",
    "    worker_vm_types=[\"t3.large\"],  # 2CPU, 8GiB\n",
    "    scheduler_vm_types=[\"t3.large\"], #maybe we should try t3.xlarge? , \n",
    "    package_sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda793f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab3cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = {\n",
    "    \"05GB\": \"s3://coiled-datasets/h2o-benchmark/N_1e7_K_1e2_parquet/*.parquet\",\n",
    "    \"5GB\": \"s3://coiled-datasets/h2o-benchmark/N_1e8_K_1e2_parquet/*.parquet\",\n",
    "    \"50GB\": \"s3://coiled-datasets/h2o-benchmark/N_1e9_K_1e2_parquet/*.parquet\",\n",
    "    \"05GB_id3NC\": \"s3://coiled-datasets/h2o-benchmark/id3_nocat/N_1e7_K_1e2_parquet/*.parquet\",\n",
    "    \"5GB_id3NC\": \"s3://coiled-datasets/h2o-benchmark/id3_nocat/N_1e8_K_1e2_parquet/*.parquet\",\n",
    "    \"50GB_id3NC\": \"s3://coiled-datasets/h2o-benchmark/id3_nocat/N_1e9_K_1e2_parquet/*.parquet\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44109817-9ae7-49bb-9fff-ff791757e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id3NC stands for id3 No Categorical, id3 is string[python]\n",
    "ds = \"50GB_id3NC\" # choose \"05GB\" , \"5GB\" or \"50GB\", 05GB_id3NC, 5GB_id3NC, 50GB_id3NC\n",
    "report_dir = \"performance-reports-no-cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee138b7-47e4-4c6b-97d6-28fdc3f8a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\n",
    "    data_size[ds],\n",
    "    engine=\"pyarrow\",\n",
    "    storage_options={\"anon\": True},\n",
    ")\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8881a3d4-645a-43f1-8b2c-35a561b1a50b",
   "metadata": {},
   "source": [
    "### Memory usage and `dtype` inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b131f692-5aaa-48c2-8123-d9abe7e50d0f",
   "metadata": {},
   "source": [
    "Total size on disk and in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc80a57-7c97-4625-a180-bab94fc5b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.utils import format_bytes\n",
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "disk_size = fs.du(os.path.dirname(data_size[ds]))\n",
    "format_bytes(disk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48d21b-eb80-42ab-a7e9-88290f72cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_size = ddf.memory_usage(deep=True).sum().compute()\n",
    "format_bytes(in_memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d952df8-4987-4f09-a8d5-35e99cf77894",
   "metadata": {},
   "source": [
    "Data is ~4x larger in memory than on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9810641-b89c-4d8d-a480-42efed2d214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.memory_usage_per_partition(deep=True).compute() / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d4d28-18bf-434b-93d8-a8c1306af1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.astype({\"id3\": \"string[pyarrow]\"}).memory_usage_per_partition(deep=True).compute() / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27cafd0-5384-4347-9fda-9c12f8cb7cb0",
   "metadata": {},
   "source": [
    "Column `id3` takes up the most memory (by far). This is a `categorical` with lots of categories. Using `string[pyarrow]` instead of `category` takes up ~2x less memory for the entire DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7e51e-66fc-4ac3-8a9e-f9860337adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.partitions[0].memory_usage(deep=True).compute() / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7879e7-5590-4838-85a0-36b2bb4dfce5",
   "metadata": {},
   "source": [
    "Converting to `string[pyarrow]` reduces memory of `id3` by 4x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa9e19-2769-4fc1-82e0-55c76a8e22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.partitions[0].astype({\"id3\": \"string[pyarrow]\"}).memory_usage(deep=True).compute() / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c760103",
   "metadata": {},
   "source": [
    "### General notes\n",
    "\n",
    "Ian mentioned that the optimization for columns might not happen when we do things like \n",
    "`ddf.astype({\"id3\": \"object\"})[[\"id3\", \"v1\", \"v3\"]]` . We might want to modify the notebook to explicitely pass the columns to the `read_parquet` call. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283b3fa0",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf6ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with performance_report(filename=os.path.join(report_dir, f\"q1_{ds}.html\")):\n",
    "    ddf_q1 = ddf[[\"id1\", \"v1\"]]\n",
    "    ddf_q1.groupby(\"id1\", dropna=False, observed=True).agg({\"v1\": \"sum\"}).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078c268",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with performance_report(filename=os.path.join(report_dir, f\"q2_{ds}.html\")):\n",
    "    \n",
    "    ddf_q2 = ddf[[\"id1\", \"id2\", \"v1\"]]\n",
    "    (\n",
    "        ddf_q2.groupby([\"id1\", \"id2\"], dropna=False, observed=True)\n",
    "        .agg({\"v1\": \"sum\"})\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525d8c18",
   "metadata": {},
   "source": [
    "### Q3 \n",
    "\n",
    "\"50GB\" got\n",
    "\n",
    "```python\n",
    "KilledWorker: (\"('aggregate-chunk-1352eeaf67172ec25f4661a39965a066-d11c92fa9cd76cfe076460d4406ad2c6', 1736)\", <WorkerState 'tls://10.0.11.26:40507', name: h2o-benchmarks-worker-e7b4d2ced4, status: closed, memory: 0, processing: 45>)\n",
    "\n",
    "WARNING:root:error sending AWS credentials to cluster: Timed out trying to connect to \n",
    "tls://10.0.10.223:38937 after 30 s\n",
    "```\n",
    "\n",
    "NOTES (\"50GB\"): \n",
    "- Using `shuffle=\"p2p\"` in the `.agg(...)` call allows this query to finish with the \"50 GB\" dataset\n",
    "- Changing `id3` to `string[python]` in the original data instead of categorical, still results in a KillWorker without `shuffle=p2p`\n",
    "- `string[python]` or `string[pyarrow]` and `shuffle=p2p` don't work at the moment. \n",
    "- Having id3 saved as `string[python]` and reading it as object using astype, the query with p2p took a little bit less than 10 min (528.19 s)\n",
    "- Using `shuffle=\"tasks\"` and casting as objects finishes in ~12 min (12m 32s) but it takes a while, probably the cluster is not big enough. \n",
    "- Using `shuffle=\"tasks\"` and NOT casting to object turns into `KilledWorker`. No idea why this is happening but it is weird that without casting `id3` as an object, results in worst performance than pure strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460074ec-1c2e-4f64-8bdd-42689c039cef",
   "metadata": {},
   "source": [
    "Some notes from looking at query 3 with the 5 GM dataset:\n",
    "\n",
    "- Categoricals are less efficient than `string[python]` and `string[pyarrow]` types. At least when there are lots of categories. \n",
    "\n",
    "- Want to make it easier for the user to specify dtypes in `read_parquet`\n",
    "\n",
    "- Can't use `shuffle=\"p2p\"` with `string[pyarrow]`. Breaks for some reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e2a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original query 3\n",
    "with performance_report(filename=os.path.join(report_dir, f\"q3_{ds}.html\")):\n",
    "    \n",
    "    ddf_q3 = ddf[[\"id3\", \"v1\", \"v3\"]]\n",
    "    (\n",
    "        ddf_q3.groupby(\"id3\", dropna=False, observed=True)\n",
    "        .agg({\"v1\": \"sum\", \"v3\": \"mean\"})\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72547cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3 id3 is saved as string[python] try shuffle=\"tasks\"\n",
    "with performance_report(filename=os.path.join(report_dir, f\"q3_{ds}-tasks-shuffle_id3nocat_obj.html\")):\n",
    "    \n",
    "\n",
    "    ddf_q3 = ddf.astype({\"id3\": \"object\"})[[\"id3\", \"v1\", \"v3\"]]\n",
    "    (\n",
    "        ddf_q3.groupby(\"id3\", dropna=False, observed=True)\n",
    "        .agg({\"v1\": \"sum\", \"v3\": \"mean\"}, shuffle=\"tasks\")\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611afb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3 id3 is saved as string[python] but does not work with p2p, try astype object\n",
    "with performance_report(filename=os.path.join(report_dir, f\"q3_{ds}-p2p-shuffle_id3object.html\")):\n",
    "    \n",
    "\n",
    "    ddf_q3 = ddf.astype({\"id3\": \"object\"})[[\"id3\", \"v1\", \"v3\"]]\n",
    "    (\n",
    "        ddf_q3.groupby(\"id3\", dropna=False, observed=True)\n",
    "        .agg({\"v1\": \"sum\", \"v3\": \"mean\"}, shuffle=\"p2p\")\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006d5e7-3dd8-4c3f-8d05-9c55bd9e6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3 with `p2p` shuffling algorithm\n",
    "with performance_report(filename=os.path.join(report_dir, f\"q3_{ds}-p2p-shuffle.html\")):\n",
    "    \n",
    "    ddf_q3 = ddf[[\"id3\", \"v1\", \"v3\"]]\n",
    "    (\n",
    "        ddf_q3.groupby(\"id3\", dropna=False, observed=True)\n",
    "        .agg({\"v1\": \"sum\", \"v3\": \"mean\"}, shuffle=\"p2p\")\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ec0c8-cf20-47ec-b47a-cebbb787d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3 with `string[pyarrow]` dtype\n",
    "with performance_report(filename=os.path.join(report_dir, f\"q3_{ds}-pyarrow.html\")):\n",
    "    \n",
    "    ddf_q3 = ddf.astype({\"id3\": \"string[pyarrow]\"})[[\"id3\", \"v1\", \"v3\"]]\n",
    "    (\n",
    "        ddf_q3.groupby(\"id3\", dropna=False, observed=True)\n",
    "        .agg({\"v1\": \"sum\", \"v3\": \"mean\"})\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9766cf6-cd35-41ee-9677-11b80e36266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3 with `string[python]` dtype\n",
    "with performance_report(filename=os.path.join(report_dir, f\"q3_{ds}-python.html\")):\n",
    "    \n",
    "    ddf_q3 = ddf.astype({\"id3\": \"string[python]\"})[[\"id3\", \"v1\", \"v3\"]]\n",
    "    (\n",
    "        ddf_q3.groupby(\"id3\", dropna=False, observed=True)\n",
    "        .agg({\"v1\": \"sum\", \"v3\": \"mean\"})\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c659e09",
   "metadata": {},
   "source": [
    "### Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80abc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with performance_report(filename=os.path.join(report_dir, f\"q4_{ds}.html\")):\n",
    "    \n",
    "    ddf_q4 = ddf[[\"id4\", \"v1\", \"v2\", \"v3\"]]\n",
    "    (\n",
    "        ddf_q4.groupby(\"id4\", dropna=False, observed=True)\n",
    "        .agg({\"v1\": \"mean\", \"v2\": \"mean\", \"v3\": \"mean\"})\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2476aea",
   "metadata": {},
   "source": [
    "### Q5\n",
    "50GB we see some spilling about 5GB of spilling, some workers dying but restarting very close to finishing, untile we got a:\n",
    "\n",
    "```python\n",
    "KilledWorker: (\"('aggregate-combine-0c4783a5d5d1ec1968c74fb8baec15e9', 0, 3)\", <WorkerState 'tls://10.0.15.17:36179', name: h2o-benchmarks-worker-7d835dad59, status: closed, memory: 0, processing: 1>)\n",
    "```\n",
    "\n",
    "By looking at the data generations this is another case of high-cardinality but with integers. From the script,\n",
    "```python\n",
    "    data[\"id6\"] = np.random.choice(int(num_rows / num_groups), size=size, replace=True)\n",
    "```\n",
    "- On 50GB and shuffle p2p the query running it as:\n",
    "```python\n",
    "ddf_q5 = ddf[[\"id6\", \"v1\", \"v2\", \"v3\"]]\n",
    "    (\n",
    "        ddf_q5.groupby(\"id6\", dropna=False, observed=True)\n",
    "        .agg({\"v1\": \"sum\", \"v2\": \"sum\", \"v3\": \"sum\"}, shuffle=\"p2p\")\n",
    "        .compute()\n",
    "    )\n",
    "```\n",
    "\n",
    "returns \n",
    "\n",
    "```\n",
    "AttributeError: 'Index' object has no attribute 'head'\n",
    "```\n",
    "- On 50GB and shuffle p2p cating id6 to \"int32\" finishes in ~4 min (249.57 s)\n",
    "\n",
    "- On 50GB and shuffle tasks finishes in ~6 min (364.40 s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with performance_report(filename=os.path.join(report_dir, f\"q5_{ds}.html\")):\n",
    "    \n",
    "    ddf_q5 = ddf[[\"id6\", \"v1\", \"v2\", \"v3\"]]\n",
    "    (\n",
    "        ddf_q5.groupby(\"id6\", dropna=False, observed=True)\n",
    "        .agg({\"v1\": \"sum\", \"v2\": \"sum\", \"v3\": \"sum\"})\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8455a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AttributeError: 'Index' object has no attribute 'head' if reading data - known issue reported in dask #9476\n",
    "# ddf_q5 = ddf[[\"id6\", \"v1\", \"v2\", \"v3\"]]\n",
    "with performance_report(filename=os.path.join(report_dir, f\"q5_{ds}_shuffle_p2p.html\")):\n",
    "    \n",
    "    ddf_q5 = ddf.astype({\"id6\": \"int32\"})[[\"id6\", \"v1\", \"v2\", \"v3\"]]\n",
    "    (\n",
    "        ddf_q5.groupby(\"id6\", dropna=False, observed=True)\n",
    "        .agg({\"v1\": \"sum\", \"v2\": \"sum\", \"v3\": \"sum\"}, shuffle=\"p2p\")\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with performance_report(filename=os.path.join(report_dir, f\"q5_{ds}_shuffle_tasks.html\")):\n",
    "    \n",
    "    ddf_q5 = ddf[[\"id6\", \"v1\", \"v2\", \"v3\"]]\n",
    "    (\n",
    "        ddf_q5.groupby(\"id6\", dropna=False, observed=True)\n",
    "        .agg({\"v1\": \"sum\", \"v2\": \"sum\", \"v3\": \"sum\"}, shuffle=\"tasks\")\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec0880b",
   "metadata": {},
   "source": [
    "### Q7 \n",
    "50GB \n",
    "```python\n",
    "KilledWorker: (\"('aggregate-chunk-351c6302b956751c118f4e66ba112f29-25ffa2e10908bad5aab7bf2896ccaca1', 1742)\", <WorkerState 'tls://10.0.10.247:40875', name: h2o-benchmarks-worker-fa206d90a7, status: closed, memory: 0, processing: 40>)\n",
    "```\n",
    "Notes:\n",
    "- Having id3 saved as `string[python]` and reading it as object using astype, the query with p2p took 472.83 s (~8 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with performance_report(filename=os.path.join(report_dir, f\"q7_{ds}.html\")):\n",
    "    \n",
    "    ddf_q7 = ddf[[\"id3\", \"v1\", \"v2\"]]\n",
    "    (\n",
    "        ddf_q7.groupby(\"id3\", dropna=False, observed=True)\n",
    "        .agg({\"v1\": \"max\", \"v2\": \"min\"})\n",
    "        .assign(range_v1_v2=lambda x: x[\"v1\"] - x[\"v2\"])[[\"range_v1_v2\"]]\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56197648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 with p2p and id3 astype object\n",
    "with performance_report(filename=os.path.join(report_dir, f\"q7_{ds}-p2p-shuffle_id3object.html\")):\n",
    "    \n",
    "    ddf_q7 = ddf.astype({\"id3\": \"object\"})[[\"id3\", \"v1\", \"v2\"]]\n",
    "    (\n",
    "        ddf_q7.groupby(\"id3\", dropna=False, observed=True)\n",
    "        .agg({\"v1\": \"max\", \"v2\": \"min\"}, shuffle=\"p2p\")\n",
    "        .assign(range_v1_v2=lambda x: x[\"v1\"] - x[\"v2\"])[[\"range_v1_v2\"]]\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b56fa",
   "metadata": {},
   "source": [
    "### Q8\n",
    "Takes a long time in general. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c2b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with performance_report(filename=os.path.join(report_dir, f\"q8_{ds}.html\")):\n",
    "    \n",
    "    ddf_q8 = ddf[[\"id6\", \"v1\", \"v2\", \"v3\"]]\n",
    "    (\n",
    "        ddf_q8[~ddf_q8[\"v3\"].isna()][[\"id6\", \"v3\"]]\n",
    "        .groupby(\"id6\", dropna=False, observed=True)\n",
    "        .apply(\n",
    "            lambda x: x.nlargest(2, columns=\"v3\"),\n",
    "            meta={\"id6\": \"Int64\", \"v3\": \"float64\"},\n",
    "        )[[\"v3\"]]\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971dfca4",
   "metadata": {},
   "source": [
    "### Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with performance_report(filename=os.path.join(report_dir, f\"q9_{ds}.html\")):\n",
    "    \n",
    "    ddf_q9 = ddf[[\"id2\", \"id4\", \"v1\", \"v2\"]]\n",
    "    (\n",
    "        ddf_q9[[\"id2\", \"id4\", \"v1\", \"v2\"]]\n",
    "        .groupby([\"id2\", \"id4\"], dropna=False, observed=True)\n",
    "        .apply(\n",
    "            lambda x: pd.Series({\"r2\": x.corr()[\"v1\"][\"v2\"] ** 2}),\n",
    "            meta={\"r2\": \"float64\"},\n",
    "        )\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a1392-b004-4c19-a1c7-fc2d24d9a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86fd91-0654-40a4-b2ea-6b53c37a8db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
